{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:47:38.824099Z",
     "start_time": "2025-11-10T04:47:30.530763Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "960f40373cd76979",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-10T04:47:38.862642Z",
     "start_time": "2025-11-10T04:47:38.839611Z"
    }
   },
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Tối ưu hóa bằng cách:\n",
    "    - Sử dụng F.scaled_dot_product_attention (Flash Attention trong PyTorch 2.0+)\n",
    "    - Giảm số phép tính lại không cần thiết\n",
    "    - Giảm số biến trung gian tạm thời\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads=4, dropout=0.1, at_mask=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emb_dim: Kích thước embedding\n",
    "            num_heads: Số lượng đầu attention\n",
    "            dropout: Tỷ lệ dropout\n",
    "            at_mask: Nếu True, sử dụng causal mask (dùng cho decoder)\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert emb_dim % num_heads == 0, \"emb_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads if emb_dim % num_heads == 0 else emb_dim // (num_heads - 1)\n",
    "        self.at_mask = at_mask\n",
    "\n",
    "        self.QKV_linear = nn.Linear(emb_dim, emb_dim * 3, bias=False)\n",
    "        self.out_linear = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, emb_dim) - input tensor\n",
    "            mask: (attn_mask) (seq_len, seq_len) or (batch_size, seq_len, seq_len) khi evn có PyTorch 2.0+\n",
    "        Return:\n",
    "           (batch_size, seq_len, emb_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, emb_dim = x.size()\n",
    "        qkv = self.QKV_linear(x)  # (batch_size, seq_len, emb_dim * 3)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_len, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        if hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            attn_output = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                attn_mask=mask,\n",
    "                dropout_p=self.dropout if self.training else 0.0,\n",
    "                is_causal=self.at_mask\n",
    "            )  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        #      bx = 2 yêu cầu phải có mask dạng (batch_size, seq_len, seq_len)\n",
    "        else:\n",
    "            # Fallback to manual implementation\n",
    "            scale = 1.0 / math.sqrt(self.head_dim)\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "\n",
    "            if self.atmask:\n",
    "                mask = torch.triu(torch.ones((seq_len, seq_len), device=x.device), diagonal=1).bool()\n",
    "                scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "            attn = F.softmax(scores, dim=-1)\n",
    "            attn = F.dropout(attn, p=self.dropout_p, training=self.training)\n",
    "            attn_output = torch.matmul(attn, v)\n",
    "\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()  # (batch_size, seq_len, num_heads, head_dim)\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, emb_dim)  # (batch_size, seq_len, emb_dim)\n",
    "        attn_output = self.out_linear(attn_output)  # (batch_size, seq_len, emb_dim)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Tối ưu hóa bằng cách:\n",
    "    - Cache positional encoding (không tính lại mỗi forward)\n",
    "    - Sử dụng register_buffer để tự động chuyển device\n",
    "    - Broadcasting thay vì repeat\n",
    "    - Pre-compute trong __init__\n",
    "    - Thêm dropout cho regularization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim=512, max_seq_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Kích thước từ vựng\n",
    "            emb_dim: Kích thước embedding\n",
    "            max_seq_len: Độ dài chuỗi tối đa\n",
    "            dropout: Tỷ lệ dropout\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        # Pre-compute positional encoding và cache\n",
    "        pe = self._create_positional_encoding(max_seq_len, emb_dim)\n",
    "        # Register as buffer (không train, tự động chuyển device)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def _create_positional_encoding(self, max_seq_len, emb_dim):\n",
    "        \"\"\"Pre-compute positional encoding một lần duy nhất\"\"\"\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Tối ưu công thức div_term\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, emb_dim, 2).float() * (-math.log(10000.0) / emb_dim)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        return pe.unsqueeze(0)  # (1, max_seq_len, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len) - token indices\n",
    "        Returns:\n",
    "            (batch_size, seq_len, emb_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        # Token embedding\n",
    "        token_emb = self.token_emb(x)  # (batch_size, seq_len, emb_dim)\n",
    "\n",
    "        # Positional encoding - sử dụng broadcasting (không cần repeat)\n",
    "        pos_emb = self.pe[:, :seq_len, :]  # (1, seq_len, emb_dim)\n",
    "\n",
    "        # Combine (broadcasting tự động)\n",
    "        x = token_emb + pos_emb\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    \"\"\"\n",
    "    Tối ưu hóa bằng cách:\n",
    "        - Sử dụng SwiGLU thay vì ReLU\n",
    "        - Thêm dropout cho regularization\n",
    "    Ở đây sử dụng SwiGLU: SwiGLU(x) = (Swish(W1(x)) ⊙ W3(x)) W2\n",
    "       where Swish(x) = x * sigmoid(x) = SiLU(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model=512, d_ff=None, dropout=0.1, bias=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Kích thước embedding đầu vào\n",
    "            d_ff: Kích thước ẩn của feedforward network\n",
    "            dropout: Tỷ lệ dropout\n",
    "            bias: Sử dụng bias trong các lớp Linear hay không\n",
    "        \"\"\"\n",
    "        super(Feedforward, self).__init__()\n",
    "\n",
    "        if d_ff is None:\n",
    "            # LLaMA uses 8/3 * d_model to compensate for gating\n",
    "            d_ff = int(8 * d_model / 3)\n",
    "            # Round to nearest multiple of 256 for efficiency\n",
    "            d_ff = 256 * ((d_ff + 255) // 256)\n",
    "\n",
    "        # Three projections: W1 (gate), W2 (down), W3 (up)\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=bias)  # Gate\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=bias)  # Down projection\n",
    "        self.w3 = nn.Linear(d_model, d_ff, bias=bias)  # Up projection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # SwiGLU(x) = (Swish(W1(x)) ⊙ W3(x)) W2\n",
    "        # where Swish(x) = x * sigmoid(x) = SiLU(x)\n",
    "        gate = F.silu(self.w1(x))\n",
    "        x = self.w3(x)\n",
    "        x = gate * x  # Element-wise multiplication\n",
    "        x = self.dropout(x)\n",
    "        x = self.w2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    tối ưu hóa bằng cách:\n",
    "        - Sử dụng F.scaled_dot_product_attention (Flash Attention trong PyTorch 2.0+)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads=8, dropout=0.1, bias=True):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        assert emb_dim % num_heads == 0, \"emb_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "        # Projections\n",
    "        self.query_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.key_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.value_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "\n",
    "    def forward(self, query, key_value, attn_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch_size, seq_len_q, emb_dim) - from decoder\n",
    "            key_value: (batch_size, seq_len_kv, emb_dim) - from encoder\n",
    "            attn_mask:  (seq_len_q, seq_len_kv) or (batch_size, seq_len_q, seq_len_kv) khi evn có PyTorch 2.0+\n",
    "            key_padding_mask: (batch_size, seq_len_kv) - True for positions to ignore khi evn có PyTorch < 2.0\n",
    "        Returns:\n",
    "            (batch_size, seq_len_q, emb_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len_q, _ = query.size()\n",
    "        seq_len_kv = key_value.size(1)\n",
    "\n",
    "        # Project and reshape\n",
    "        Q = self.query_proj(query).view(batch_size, seq_len_q, self.num_heads, self.head_dim)\n",
    "        K = self.key_proj(key_value).view(batch_size, seq_len_kv, self.num_heads, self.head_dim)\n",
    "        V = self.value_proj(key_value).view(batch_size, seq_len_kv, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to (batch_size, num_heads, seq_len, head_dim)\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Use PyTorch's optimized scaled_dot_product_attention\n",
    "        if hasattr(F, 'scaled_dot_product_attention'):\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                Q, K, V,\n",
    "                attn_mask=attn_mask,\n",
    "                dropout_p=self.dropout_p if self.training else 0.0,\n",
    "                is_causal=False  # Cross-attention is not causal\n",
    "            )\n",
    "        else:\n",
    "            # Fallback\n",
    "            scale = 1.0 / math.sqrt(self.head_dim)\n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "                scores = scores.masked_fill(key_padding_mask, float('-inf'))\n",
    "\n",
    "            if attn_mask is not None:\n",
    "                scores = scores + attn_mask\n",
    "\n",
    "            attn = F.softmax(scores, dim=-1)\n",
    "            attn = F.dropout(attn, p=self.dropout_p, training=self.training)\n",
    "            out = torch.matmul(attn, V)\n",
    "\n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        out = out.view(batch_size, seq_len_q, self.emb_dim)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder block gồm:\n",
    "    - Multi-Head Attention với residual connection và layer normalization\n",
    "    - Feedforward network với residual connection và layer normalization\n",
    "\n",
    "    Diagram of Encoder block:\n",
    "\n",
    "    Encoder\n",
    "    ├────────────────────────────────┐\n",
    "    ├───Multi-Head Attention         │\n",
    "    │   └───Head Attention x 4       │\n",
    "    ├ + <────────────────────────────┘\n",
    "    │\n",
    "    ├───nn.LayerNorm\n",
    "    ├────────────────────────────────┐\n",
    "    ├───Feedforward                  │\n",
    "    ├ + <────────────────────────────┘\n",
    "    └───nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dmodel=512, num_heads=4, d_ff=None, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dmodel: Kích thước embedding\n",
    "            num_heads: Số lượng head attention\n",
    "            dropout: Tỷ lệ dropout\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mha = MultiHeadAttention(emb_dim=dmodel, num_heads=num_heads, dropout=dropout)\n",
    "        self.ffn = Feedforward(d_model=dmodel, dropout=dropout, d_ff=d_ff)\n",
    "        self.norm1 = nn.LayerNorm(dmodel)\n",
    "        self.norm2 = nn.LayerNorm(dmodel)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, dmodel)\n",
    "            mask:  (batch_size, seq_len, seq_len) khi evn có PyTorch 2.0+\n",
    "        Returns:\n",
    "            (batch_size, seq_len, dmodel)\n",
    "        \"\"\"\n",
    "        # Multi-Head Attention with Residual Connection\n",
    "        attn_output = self.mha(x, mask=mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Feedforward Network with Residual Connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block gồm:\n",
    "    - Masked Multi-Head Attention với residual connection và layer normalization\n",
    "    - Cross-Attention với residual connection và layer normalization\n",
    "    - Feedforward network với residual connection và layer normalization\n",
    "    Diagram of Decoder block:\n",
    "\n",
    "    Decoder\n",
    "    ├────────────────────────────────┐\n",
    "    ├───Masked Multi-Head Attention  │\n",
    "    │   └───Head Attention x 4       │\n",
    "    ├ + <────────────────────────────┘\n",
    "    │\n",
    "    ├───nn.LayerNorm\n",
    "    ├────────────────────────────────┐\n",
    "    ├───Cross-Attention              │\n",
    "    │   └───Head Attention x 4       │\n",
    "    ├ + <────────────────────────────┘\n",
    "    │\n",
    "    ├───nn.LayerNorm\n",
    "    ├────────────────────────────────┐\n",
    "    ├───Feedforward                  │\n",
    "    ├ + <────────────────────────────┘\n",
    "    └───nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dmodel=512, num_heads=4, d_ff=None, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dmodel : Kích thước embedding\n",
    "            num_heads : Số lượng head attention\n",
    "            dropout : Tỷ lệ dropout\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mha = MultiHeadAttention(emb_dim=dmodel, num_heads=num_heads, dropout=dropout, at_mask=True)\n",
    "        self.cross_attn = CrossAttention(emb_dim=dmodel, num_heads=num_heads, dropout=dropout)\n",
    "        self.ffn = Feedforward(d_model=dmodel, dropout=dropout, d_ff=d_ff)\n",
    "        self.norm1 = nn.LayerNorm(dmodel)\n",
    "        self.norm2 = nn.LayerNorm(dmodel)\n",
    "        self.norm3 = nn.LayerNorm(dmodel)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask=None, src_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: (batch_size, tgt_seq_len, dmodel) - input tensor to decoder\n",
    "            enc_output: (batch_size, src_seq_len, dmodel) - output from encoder\n",
    "            src_mask: (batch_size, tgt_seq_len, src_seq_len) khi evn có PyTorch 2.0+\n",
    "            tgt_mask: (batch_size, src_seq_len) khi evn có PyTorch < 2.0\n",
    "        Returns:\n",
    "            (batch_size, tgt_seq_len, dmodel)\n",
    "        \"\"\"\n",
    "        # Masked Multi-Head Attention with Residual Connection\n",
    "        attn_output = self.mha(x, mask=tgt_mask)  # multi-head self-attention\n",
    "        x = x + self.dropout(attn_output)  # residual connection\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Cross-Attention with Residual Connection\n",
    "        cross_attn_output = self.cross_attn(x, enc_output, attn_mask=src_mask,\n",
    "                                            key_padding_mask=tgt_mask)  # cross-attention\n",
    "        x = x + self.dropout(cross_attn_output)  # residual connection\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # Feedforward Network with Residual Connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:47:39.522698Z",
     "start_time": "2025-11-10T04:47:39.511698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "       Optimized Transformer với:\n",
    "       - Pre-LayerNorm\n",
    "       - Optimized components\n",
    "       - Proper masking\n",
    "       - Gradient checkpointing support\n",
    "       - Mixed precision support\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 d_model=512,\n",
    "                 num_heads=8,\n",
    "                 num_encoder_layers=6,\n",
    "                 num_decoder_layers=6,\n",
    "                 d_ff=2048,\n",
    "                 dropout=0.1,\n",
    "                 max_seq_len=5000,\n",
    "                 pad_idx=(0, 1),\n",
    "                 use_gradient_checkpointing=False):\n",
    "        \"\"\"\n",
    "        Khởi tạo mô hình Transformer\n",
    "            Args:\n",
    "                src_vocab_size (int): Kích thước từ vựng nguồn\n",
    "                tgt_vocab_size (int): Kích thước từ vựng đích\n",
    "                d_model (int): Kích thước embedding và mô hình\n",
    "                num_heads (int): Số lượng đầu attention\n",
    "                num_encoder_layers (int): Số lớp encoder\n",
    "                num_decoder_layers (int): Số lớp decoder\n",
    "                d_ff (int): Kích thước của feed-forward layer\n",
    "                dropout (float): Tỷ lệ dropout\n",
    "                max_seq_len (int): Độ dài tối đa của chuỗi\n",
    "                pad_idx (tuple): Chỉ số padding trong từ vựng\n",
    "                use_gradient_checkpointing (bool): Sử dụng gradient checkpointing để tiết kiệm bộ nhớ\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        # prame init\n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        self.use_gradient_checkpointing = use_gradient_checkpointing\n",
    "\n",
    "        # embedding\n",
    "        self.src_embedding = Embedding(src_vocab_size, d_model, max_seq_len, dropout)\n",
    "        self.tgt_embedding = Embedding(tgt_vocab_size, d_model, max_seq_len, dropout)\n",
    "        # encoder\n",
    "        self.encoder = nn.ModuleList([\n",
    "            Encoder(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        self.encoder_norm = nn.LayerNorm(d_model)\n",
    "        # decoder\n",
    "        self.decoder = nn.ModuleList([\n",
    "            Decoder(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        self.decoder_norm = nn.LayerNorm(d_model)\n",
    "        # prediction head\n",
    "        self.output_layer = nn.Linear(d_model, tgt_vocab_size, bias=False)\n",
    "        self.output_layer.weight = self.tgt_embedding.token_emb.weight\n",
    "        # reset parameters\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Initialize parameters\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, tgt, mask_src=None, mask_tgt=None):\n",
    "        if mask_src is None:\n",
    "            mask_src = self.make_src_mask(src)\n",
    "        if mask_tgt is None:\n",
    "            mask_tgt = self.make_tgt_mask(tgt)\n",
    "\n",
    "        encoder_out = self.encode(src, mask_src=mask_src)\n",
    "        decoder_out = self.decode(tgt=tgt, enc_src=encoder_out, mask_tgt=mask_tgt, mask_src=mask_src)\n",
    "        out_puts = self.output_layer(decoder_out)\n",
    "        return out_puts\n",
    "\n",
    "    def encode(self, src, mask_src=None):\n",
    "        \"\"\"Encode source sequence\n",
    "        Arg:\n",
    "            src: (batch_size, src_len)\n",
    "            mask_src: (batch_size, 1, 1, src_len)\n",
    "        \"\"\"\n",
    "        # src: (batch_size, src_len)\n",
    "        # mask_src: (batch_size, 1, 1, src_len)\n",
    "        src = self.src_embedding(src)  # (batch_size, src_len, d_model)\n",
    "        for layer in self.encoder:\n",
    "            if self.use_gradient_checkpointing:\n",
    "                src = torch.utils.checkpoint.checkpoint(layer, src, mask_src)  # No tgt for encoder\n",
    "            else:\n",
    "                src = layer(src, mask_src)\n",
    "        src = self.encoder_norm(src)\n",
    "        return src\n",
    "\n",
    "    def decode(self, tgt, enc_src, mask_tgt=None, mask_src=None):\n",
    "        \"\"\"Decode target sequence\n",
    "        Arg:\n",
    "            tgt: (batch_size, tgt_len)\n",
    "            enc_src: (batch_size, src_len, d_model)\n",
    "            mask_tgt: (batch_size, 1, tgt_len, tgt_len)\n",
    "            mask_src: (batch_size, 1, 1, src_len)\n",
    "        \"\"\"\n",
    "\n",
    "        tgt = self.tgt_embedding(tgt)  # (batch_size, tgt_len, d_model)\n",
    "        for layer in self.decoder:\n",
    "            if self.use_gradient_checkpointing:\n",
    "                tgt = torch.utils.checkpoint.checkpoint(\n",
    "                    layer, tgt, enc_src, mask_tgt, mask_src\n",
    "                )\n",
    "            else:\n",
    "                tgt = layer(tgt, enc_src, mask_tgt, mask_src)\n",
    "        tgt = self.decoder_norm(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"Create source padding mask\"\"\"\n",
    "        # src: (batch_size, src_len)\n",
    "        src_mask = (src == self.pad_idx[0]).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        \"\"\"Create target padding mask\"\"\"\n",
    "        # tgt: (batch_size, tgt_len)\n",
    "        tgt_mask = (tgt == self.pad_idx[1]).unsqueeze(1).unsqueeze(2)  # (batch_size,tgt_len)\n",
    "\n",
    "        return tgt_mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, src, max_len=50, start_token=1, end_token=2,\n",
    "                 temperature=1.0, top_k=None, top_p=None):\n",
    "        \"\"\"Generate sequence using greedy decoding or sampling\n",
    "        Args:\n",
    "            src: (batch_size, src_len)\n",
    "            max_len: chiều dài tối đa của chuỗi được tạo\n",
    "            start_token: index của token bắt đầu\n",
    "            end_token: index của token kết thúc\n",
    "            temperature: nhiệt độ cho sampling\n",
    "            top_k: top-k sampling\n",
    "            top_p: nucleus sampling\n",
    "        Returns:\n",
    "            generated sequences: (batch_size, generated_len)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = src.device\n",
    "\n",
    "        # Encode source\n",
    "        if src.dim() == 1:\n",
    "            src = src.unsqueeze(0)\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        enc_src = self.encode(src, src_mask)\n",
    "\n",
    "        tgt = torch.tensor([[start_token]], device=device)\n",
    "        for _ in range(max_len):\n",
    "            tgt_mask = self.make_tgt_mask(tgt)\n",
    "            dec_out = self.decode(tgt, enc_src, mask_src=src_mask, mask_tgt=tgt_mask)  # (batch_size, tgt_len, d_model)\n",
    "            logits = self.output_layer(dec_out[:, -1, :])  # (batch_size, vocab_size)\n",
    "            logits = logits / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Append to sequence\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "\n",
    "            # Check for end token\n",
    "            if next_token.item() == end_token:\n",
    "                break\n",
    "\n",
    "        return tgt.squeeze(0)  # (generated_len,)\n"
   ],
   "id": "29e9ed43867e978e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:47:39.543492Z",
     "start_time": "2025-11-10T04:47:39.531929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class METTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Optimized version với:\n",
    "    - Pre-filtering data quá dài\n",
    "    - Cached tokenization\n",
    "    - Proper error handling\n",
    "    - Memory efficient\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data: List[Dict[str, str]],\n",
    "            tokenizer_eng: str = \"bert-base-uncased\",\n",
    "            tokenizer_vie: str = \"vinai/phobert-base\",\n",
    "            max_length: int = 75,\n",
    "            cache_dir: Optional[str] = None,\n",
    "            use_cache: bool = True\n",
    "    ):\n",
    "        self.max_length = max_length\n",
    "        self.cache_dir = cache_dir\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        # Load tokenizers\n",
    "        logger.info(\"Loading tokenizers...\")\n",
    "        self.tokenizer_eng = AutoTokenizer.from_pretrained(tokenizer_eng)\n",
    "        self.tokenizer_vie = AutoTokenizer.from_pretrained(tokenizer_vie)\n",
    "\n",
    "        # Cache file path\n",
    "        cache_file = None\n",
    "        if cache_dir and use_cache:\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "            cache_file = os.path.join(\n",
    "                cache_dir,\n",
    "                f\"cached_data_{tokenizer_eng.replace('/', '_')}_{tokenizer_vie.replace('/', '_')}_{max_length}.pkl\"\n",
    "            )\n",
    "\n",
    "        # Try to load from cache\n",
    "        if cache_file and os.path.exists(cache_file):\n",
    "            logger.info(f\"Loading cached data from {cache_file}\")\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                cache_data = pickle.load(f)\n",
    "                self.data = cache_data['data']\n",
    "                self.en_tokens = cache_data['en_tokens']\n",
    "                self.vi_tokens = cache_data['vi_tokens']\n",
    "            logger.info(f\"Loaded {len(self.data)} samples from cache\")\n",
    "        else:\n",
    "            # Process and filter data\n",
    "            logger.info(\"Processing and filtering data...\")\n",
    "            self.data, self.en_tokens, self.vi_tokens = self._process_data(data)\n",
    "\n",
    "            # Save to cache\n",
    "            if cache_file:\n",
    "                logger.info(f\"Saving to cache: {cache_file}\")\n",
    "                with open(cache_file, 'wb') as f:\n",
    "                    pickle.dump({\n",
    "                        'data': self.data,\n",
    "                        'en_tokens': self.en_tokens,\n",
    "                        'vi_tokens': self.vi_tokens\n",
    "                    }, f)\n",
    "\n",
    "        logger.info(f\"Dataset ready with {len(self.data)} samples\")\n",
    "\n",
    "    def _process_data(self, raw_data: List[Dict[str, str]]) -> Tuple[List[Dict], List[List[int]], List[List[int]]]:\n",
    "        \"\"\"Process and filter data, return valid samples only\"\"\"\n",
    "        valid_data = []\n",
    "        en_tokens_list = []\n",
    "        vi_tokens_list = []\n",
    "\n",
    "        filtered_count = 0\n",
    "        error_count = 0\n",
    "\n",
    "        for idx, item in enumerate(tqdm(raw_data, desc=\"Tokenizing\")):\n",
    "            try:\n",
    "                en_text = item.get(\"en\", \"\")\n",
    "                vi_text = item.get(\"vi\", \"\")\n",
    "\n",
    "                # Skip empty\n",
    "                if not en_text or not vi_text:\n",
    "                    filtered_count += 1\n",
    "                    continue\n",
    "\n",
    "                # Tokenize\n",
    "                en_encoded = self.tokenizer_eng(\n",
    "                    en_text,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=False,\n",
    "                    return_attention_mask=False\n",
    "                )[\"input_ids\"]\n",
    "\n",
    "                vi_encoded = self.tokenizer_vie(\n",
    "                    vi_text,\n",
    "                    add_special_tokens=True,\n",
    "                    truncation=False,\n",
    "                    return_attention_mask=False\n",
    "                )[\"input_ids\"]\n",
    "\n",
    "                # Filter by length\n",
    "                if len(en_encoded) >= self.max_length or len(vi_encoded) >= self.max_length:\n",
    "                    filtered_count += 1\n",
    "                    continue\n",
    "\n",
    "                # Keep valid samples\n",
    "                valid_data.append(item)\n",
    "                en_tokens_list.append(en_encoded)\n",
    "                vi_tokens_list.append(vi_encoded)\n",
    "\n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                if error_count <= 5:  # Log first 5 errors\n",
    "                    logger.warning(f\"Error processing item {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Filtered {filtered_count} samples (too long or empty)\")\n",
    "        logger.info(f\"Errors: {error_count} samples\")\n",
    "        logger.info(f\"Valid samples: {len(valid_data)}\")\n",
    "\n",
    "        return valid_data, en_tokens_list, vi_tokens_list\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return pre-tokenized data\"\"\"\n",
    "        en_tokens = self.en_tokens[idx]\n",
    "        vi_tokens = self.vi_tokens[idx]\n",
    "\n",
    "        return torch.tensor(en_tokens, dtype=torch.long), torch.tensor(vi_tokens, dtype=torch.long)\n",
    "\n",
    "    def decode(self, input_ids, language: str = 'eng') -> str:\n",
    "        \"\"\"Decode token ids back to text\"\"\"\n",
    "        if language == 'eng':\n",
    "            return self.tokenizer_eng.decode(input_ids, skip_special_tokens=False)\n",
    "        elif language == 'vi':\n",
    "            return self.tokenizer_vie.decode(input_ids, skip_special_tokens=False)\n",
    "        else:\n",
    "            raise ValueError(\"language must be 'eng' or 'vi'\")\n",
    "\n",
    "    def get_vocab_size(self, language: str = 'vi') -> int:\n",
    "        \"\"\"Get vocabulary size\"\"\"\n",
    "        if language == 'eng':\n",
    "            return len(self.tokenizer_eng)\n",
    "        elif language == 'vi':\n",
    "            return len(self.tokenizer_vie)\n",
    "        else:\n",
    "            raise ValueError(\"language must be 'eng' or 'vi'\")\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor]], pad_idx_eng: int, pad_idx_vie: int) -> Dict[\n",
    "    str, torch.Tensor]:\n",
    "    \"\"\"Collate function to pad sequences in a batch\"\"\"\n",
    "    en_batch, vi_batch = zip(*batch)\n",
    "\n",
    "    en_padded = pad_sequence(en_batch, batch_first=True, padding_value=pad_idx_eng)\n",
    "    vi_padded = pad_sequence(vi_batch, batch_first=True, padding_value=pad_idx_vie)\n",
    "\n",
    "    return {\n",
    "        'en_input_ids': en_padded,\n",
    "        'vi_input_ids': vi_padded\n",
    "    }\n"
   ],
   "id": "6037cea5250f0c14",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:47:39.667093Z",
     "start_time": "2025-11-10T04:47:39.659654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized Cross Entropy Loss với:\n",
    "    - Ignore padding tokens\n",
    "    - Label smoothing\n",
    "    - Efficient computation\n",
    "    - Multiple metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            pad_idx: int = 0,\n",
    "            label_smoothing: float = 0.1,\n",
    "            reduction: str = 'mean',\n",
    "            ignore_index: Optional[int] = None\n",
    "    ):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_idx = pad_idx\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index if ignore_index is not None else pad_idx\n",
    "\n",
    "        # Use built-in CrossEntropyLoss with optimizations\n",
    "        self.loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=self.ignore_index,\n",
    "            label_smoothing=label_smoothing,\n",
    "            reduction=reduction\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            predict: torch.Tensor,\n",
    "            target: torch.Tensor,\n",
    "            return_metrics: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predict: (batch_size, seq_len, vocab_size)\n",
    "            target: (batch_size, seq_len)\n",
    "            return_metrics: khi mà True thì trả về thêm metrics\n",
    "        Returns:\n",
    "            loss or (loss, metrics_dict)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, vocab_size = predict.size()\n",
    "\n",
    "        # Reshape efficiently\n",
    "        predict_flat = predict.reshape(-1, vocab_size)\n",
    "        target_flat = target.reshape(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(predict_flat, target_flat)\n",
    "\n",
    "        if return_metrics:\n",
    "            with torch.no_grad():\n",
    "                metrics = self._compute_metrics(predict, target, predict_flat, target_flat)\n",
    "            return loss, metrics\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _compute_metrics(\n",
    "            self,\n",
    "            predict: torch.Tensor,\n",
    "            target: torch.Tensor,\n",
    "            predict_flat: torch.Tensor,\n",
    "            target_flat: torch.Tensor\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Compute additional metrics\"\"\"\n",
    "        # Mask for non-padding tokens\n",
    "        mask = (target_flat != self.ignore_index)\n",
    "\n",
    "        # Accuracy\n",
    "        pred_labels = predict_flat.argmax(dim=-1)\n",
    "        correct = (pred_labels == target_flat) & mask\n",
    "        accuracy = correct.sum().item() / mask.sum().item() if mask.sum() > 0 else 0.0\n",
    "\n",
    "        # Perplexity\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            log_probs = F.log_softmax(predict_flat.float(), dim=-1)\n",
    "            nll_loss = F.nll_loss(\n",
    "                log_probs,\n",
    "                target_flat,\n",
    "                ignore_index=self.ignore_index,\n",
    "                reduction='mean'\n",
    "            )\n",
    "            perplexity = torch.exp(nll_loss).item()\n",
    "\n",
    "        # Token count\n",
    "        num_tokens = mask.sum().item()\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'perplexity': perplexity,\n",
    "            'num_tokens': num_tokens\n",
    "        }"
   ],
   "id": "39163581df5308c7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:47:39.746700Z",
     "start_time": "2025-11-10T04:47:39.685348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICES = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 10\n",
    "PIN_MEMORY = True\n",
    "SMOOTHING = 0.1\n",
    "USE_AMP = False\n",
    "ACCUMULATION_STEPS = 2\n",
    "MAX_GRAD_NORM = 1.0\n",
    "MAX_LEN = 100\n",
    "NUM_WORKERS = 4\n"
   ],
   "id": "9baeccf419abaa51",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:49:20.585758Z",
     "start_time": "2025-11-10T04:47:39.756109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# configure logging\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*torch.utils.checkpoint:*\")\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def train(ct_model, loss, train_loader, optimizer, device, epoch=None):\n",
    "    ct_model.train()\n",
    "    total_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch}\" if epoch is not None else \"Training\")\n",
    "    for idx, batch in enumerate(loop):\n",
    "        en_input_ids = batch['en_input_ids'].to(device)\n",
    "        vi_input_ids = batch['vi_input_ids'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ct_model(\n",
    "            src=en_input_ids,\n",
    "            tgt=vi_input_ids\n",
    "        )\n",
    "\n",
    "        loss_value = loss(\n",
    "            predict=outputs,\n",
    "            target=vi_input_ids\n",
    "        )\n",
    "\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_value.item()\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            loop.set_postfix(epoch=epoch, loss=total_loss / (idx + 1), idx=idx)\n",
    "\n",
    "        if idx % 25 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset = load_dataset('hiimbach/mtet', cache_dir=\"/datasets\")[\"train\"]\n",
    "    mtet_dataset = METTDataset(dataset, cache_dir=\"./cache\", max_length=MAX_LEN, use_cache=True)\n",
    "    pad_idx = (mtet_dataset.tokenizer_eng.pad_token_id, mtet_dataset.tokenizer_vie.pad_token_id)\n",
    "    train_loader = DataLoader(\n",
    "        mtet_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        # num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=lambda x: collate_fn(x, pad_idx_eng=pad_idx[0], pad_idx_vie=pad_idx[1])\n",
    "    )\n",
    "    print(\"device:\", DEVICES)\n",
    "    model = Transformer(\n",
    "        src_vocab_size=mtet_dataset.get_vocab_size(language='eng'),\n",
    "        tgt_vocab_size=mtet_dataset.get_vocab_size(language='vi'),\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_seq_len=MAX_LEN,\n",
    "        dropout=0.1,\n",
    "        use_gradient_checkpointing=True,\n",
    "        pad_idx=pad_idx\n",
    "    )\n",
    "    model.to(DEVICES)\n",
    "    criterion = CrossEntropyLoss(\n",
    "        vocab_size=mtet_dataset.get_vocab_size(language='vi'),\n",
    "        label_smoothing=SMOOTHING,\n",
    "        pad_idx=pad_idx[1],\n",
    "    )\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        avg_loss = train(model, criterion, train_loader, optimizer, DEVICES, epoch)\n",
    "        logger.info(f\"Epoch [{epoch}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(model.state_dict(), f\"transformer_epoch_{epoch}.pth\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "3f8f2fa6002926ce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading tokenizers...\n",
      "INFO:__main__:Loading cached data from ./cache\\cached_data_bert-base-uncased_vinai_phobert-base_100.pkl\n",
      "INFO:__main__:Loaded 3557930 samples from cache\n",
      "INFO:__main__:Dataset ready with 3557930 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 111/889483 [01:06<147:38:39,  1.67it/s, epoch=1, idx=109, loss=9.27]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 93\u001B[0m\n\u001B[0;32m     89\u001B[0m         gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 93\u001B[0m     main()\n",
      "Cell \u001B[1;32mIn[7], line 83\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     81\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m AdamW(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mLEARNING_RATE)\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, EPOCHS \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 83\u001B[0m     avg_loss \u001B[38;5;241m=\u001B[39m train(model, criterion, train_loader, optimizer, DEVICES, epoch)\n\u001B[0;32m     84\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mEPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[1;32mIn[7], line 38\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(ct_model, loss, train_loader, optimizer, device, epoch)\u001B[0m\n\u001B[0;32m     35\u001B[0m loss_value\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     36\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 38\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_value\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     40\u001B[0m     loop\u001B[38;5;241m.\u001B[39mset_postfix(epoch\u001B[38;5;241m=\u001B[39mepoch, loss\u001B[38;5;241m=\u001B[39mtotal_loss \u001B[38;5;241m/\u001B[39m (idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), idx\u001B[38;5;241m=\u001B[39midx)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
